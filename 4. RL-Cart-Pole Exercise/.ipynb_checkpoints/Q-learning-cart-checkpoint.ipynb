{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep $Q$-learning\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use $Q$-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://github.com/openai/gym). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Number of possible actions: 2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Number of possible actions\n",
    "print('Number of possible actions:', env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2018-01-22 23:10:02,350] Making new env: CartPole-v1\n",
    "\n",
    "\n",
    "Number of possible actions: 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`.  You can see how many actions are possible from `env.action_space.n`, and to get a random action you can use `env.action_space.sample()`.  Passing in an action as an integer to `env.step` will generate the next step in the simulation.  This is general to all Gym games. \n",
    "\n",
    "In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to interact with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [] # actions that the agent selects\n",
    "rewards = [] # obtained rewards\n",
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "    action = env.action_space.sample()  # choose a random action\n",
    "    state, reward, done, _ = env.step(action) \n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the actions and rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: [0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Rewards: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print('Actions:', actions)\n",
    "print('Rewards:', rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions: [0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0]\n",
    "Rewards: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each step while the game is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right.\n",
    "\n",
    "## $Q$-Network\n",
    "\n",
    "To keep track of the action values, we'll use a neural network that accepts a state $s$ as input.  The output will be $Q$-values for each available action $a$ (i.e., the output is **all** action values $Q(s,a)$ _corresponding to the input state $s$_).\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "For this Cart-Pole game, the state has four values: the position and velocity of the cart, and the position and velocity of the pole.  Thus, the neural network has **four inputs**, one for each value in the state, and **two outputs**, one for each possible action. \n",
    "\n",
    "As explored in the lesson, to get the training target, we'll first use the context provided by the state $s$ to choose an action $a$, then simulate the game using that action. This will get us the next state, $s'$, and the reward $r$. With that, we can calculate $\\hat{Q}(s,a) = r + \\gamma \\max_{a'}{Q(s', a')}$.  Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "Below is one implementation of the $Q$-network. It uses two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class QNetwork:\n",
    "    def __init__(self, learning_rate=0.01, state_size=4, \n",
    "                 action_size=2, hidden_size=10, \n",
    "                 name='QNetwork'):\n",
    "        # state inputs to the Q-network\n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name='inputs')\n",
    "            \n",
    "            # One hot encode the actions to later choose the Q-value for the action\n",
    "            self.actions_ = tf.placeholder(tf.int32, [None], name='actions')\n",
    "            one_hot_actions = tf.one_hot(self.actions_, action_size)\n",
    "            \n",
    "            # Target Q values for training\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, [None], name='target')\n",
    "            \n",
    "            # ReLU hidden layers\n",
    "            self.fc1 = tf.contrib.layers.fully_connected(self.inputs_, hidden_size)\n",
    "            self.fc2 = tf.contrib.layers.fully_connected(self.fc1, hidden_size)\n",
    "            self.fc3 = tf.contrib.layers.fully_connected(self.fc2, hidden_size)\n",
    "\n",
    "            # Linear output layer\n",
    "            self.output = tf.contrib.layers.fully_connected(self.fc3, action_size, \n",
    "                                                            activation_fn=None)\n",
    "            \n",
    "            ### Train with loss (targetQ - Q)^2\n",
    "            # output has length 2, for two actions. This next line chooses\n",
    "            # one value from output (per row) according to the one-hot encoded actions.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maximum capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $Q$-Learning training algorithm\n",
    "\n",
    "We will use the below algorithm to train the network.  For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode $\\leftarrow 1$ **to** $M$ **do**\n",
    "  * Observe $s_0$\n",
    "  * **For** $t \\leftarrow 0$ **to** $T-1$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**\n",
    "\n",
    "You are welcome (and encouraged!) to take the time to extend this code to implement some of the improvements that we discussed in the lesson, to include fixed $Q$ targets, double DQNs, prioritized replay, and/or dueling networks.\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcement learning is the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "max_steps = 200                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 20                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ndrs\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = QNetwork(name='main', hidden_size=hidden_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the experience memory\n",
    "\n",
    "Here we re-initialize the simulation and pre-populate the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for ii in range(pretrain_length):\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below we'll train our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Total reward: 9.0 Training loss: 1.0934 Explore P: 0.9991\n",
      "Episode: 2 Total reward: 8.0 Training loss: 1.1085 Explore P: 0.9983\n",
      "Episode: 3 Total reward: 16.0 Training loss: 1.0362 Explore P: 0.9967\n",
      "Episode: 4 Total reward: 60.0 Training loss: 0.9565 Explore P: 0.9908\n",
      "Episode: 5 Total reward: 16.0 Training loss: 1.0015 Explore P: 0.9893\n",
      "Episode: 6 Total reward: 27.0 Training loss: 1.1569 Explore P: 0.9866\n",
      "Episode: 7 Total reward: 16.0 Training loss: 1.1772 Explore P: 0.9851\n",
      "Episode: 8 Total reward: 11.0 Training loss: 1.1637 Explore P: 0.9840\n",
      "Episode: 9 Total reward: 17.0 Training loss: 1.2759 Explore P: 0.9823\n",
      "Episode: 10 Total reward: 33.0 Training loss: 1.6035 Explore P: 0.9791\n",
      "Episode: 11 Total reward: 11.0 Training loss: 1.5378 Explore P: 0.9781\n",
      "Episode: 12 Total reward: 16.0 Training loss: 1.7374 Explore P: 0.9765\n",
      "Episode: 13 Total reward: 16.0 Training loss: 0.9907 Explore P: 0.9750\n",
      "Episode: 14 Total reward: 16.0 Training loss: 2.5376 Explore P: 0.9734\n",
      "Episode: 15 Total reward: 9.0 Training loss: 1.8615 Explore P: 0.9726\n",
      "Episode: 16 Total reward: 11.0 Training loss: 4.9874 Explore P: 0.9715\n",
      "Episode: 17 Total reward: 15.0 Training loss: 3.9583 Explore P: 0.9701\n",
      "Episode: 18 Total reward: 14.0 Training loss: 2.8863 Explore P: 0.9687\n",
      "Episode: 19 Total reward: 12.0 Training loss: 3.2307 Explore P: 0.9676\n",
      "Episode: 20 Total reward: 15.0 Training loss: 6.1314 Explore P: 0.9661\n",
      "Episode: 21 Total reward: 19.0 Training loss: 7.9112 Explore P: 0.9643\n",
      "Episode: 22 Total reward: 44.0 Training loss: 11.2788 Explore P: 0.9601\n",
      "Episode: 23 Total reward: 8.0 Training loss: 10.2401 Explore P: 0.9594\n",
      "Episode: 24 Total reward: 13.0 Training loss: 11.4357 Explore P: 0.9581\n",
      "Episode: 25 Total reward: 56.0 Training loss: 5.6353 Explore P: 0.9528\n",
      "Episode: 26 Total reward: 27.0 Training loss: 42.6619 Explore P: 0.9503\n",
      "Episode: 27 Total reward: 8.0 Training loss: 5.5423 Explore P: 0.9496\n",
      "Episode: 28 Total reward: 11.0 Training loss: 15.1247 Explore P: 0.9485\n",
      "Episode: 29 Total reward: 16.0 Training loss: 4.2119 Explore P: 0.9470\n",
      "Episode: 30 Total reward: 27.0 Training loss: 4.2428 Explore P: 0.9445\n",
      "Episode: 31 Total reward: 24.0 Training loss: 36.7893 Explore P: 0.9423\n",
      "Episode: 32 Total reward: 9.0 Training loss: 14.4956 Explore P: 0.9414\n",
      "Episode: 33 Total reward: 11.0 Training loss: 8.7078 Explore P: 0.9404\n",
      "Episode: 34 Total reward: 11.0 Training loss: 6.1720 Explore P: 0.9394\n",
      "Episode: 35 Total reward: 37.0 Training loss: 6.7529 Explore P: 0.9359\n",
      "Episode: 36 Total reward: 41.0 Training loss: 32.7308 Explore P: 0.9321\n",
      "Episode: 37 Total reward: 24.0 Training loss: 32.5624 Explore P: 0.9299\n",
      "Episode: 38 Total reward: 22.0 Training loss: 7.8743 Explore P: 0.9279\n",
      "Episode: 39 Total reward: 25.0 Training loss: 8.7798 Explore P: 0.9256\n",
      "Episode: 40 Total reward: 13.0 Training loss: 8.6859 Explore P: 0.9244\n",
      "Episode: 41 Total reward: 13.0 Training loss: 32.4712 Explore P: 0.9232\n",
      "Episode: 42 Total reward: 10.0 Training loss: 4.5248 Explore P: 0.9223\n",
      "Episode: 43 Total reward: 27.0 Training loss: 8.2163 Explore P: 0.9199\n",
      "Episode: 44 Total reward: 13.0 Training loss: 24.2766 Explore P: 0.9187\n",
      "Episode: 45 Total reward: 16.0 Training loss: 28.1444 Explore P: 0.9172\n",
      "Episode: 46 Total reward: 10.0 Training loss: 46.8431 Explore P: 0.9163\n",
      "Episode: 47 Total reward: 12.0 Training loss: 29.3701 Explore P: 0.9152\n",
      "Episode: 48 Total reward: 33.0 Training loss: 45.9403 Explore P: 0.9123\n",
      "Episode: 49 Total reward: 13.0 Training loss: 56.0292 Explore P: 0.9111\n",
      "Episode: 50 Total reward: 19.0 Training loss: 33.7828 Explore P: 0.9094\n",
      "Episode: 51 Total reward: 12.0 Training loss: 6.2433 Explore P: 0.9083\n",
      "Episode: 52 Total reward: 13.0 Training loss: 4.3796 Explore P: 0.9071\n",
      "Episode: 53 Total reward: 10.0 Training loss: 5.5272 Explore P: 0.9062\n",
      "Episode: 54 Total reward: 19.0 Training loss: 5.9201 Explore P: 0.9045\n",
      "Episode: 55 Total reward: 14.0 Training loss: 29.5137 Explore P: 0.9033\n",
      "Episode: 56 Total reward: 11.0 Training loss: 6.6518 Explore P: 0.9023\n",
      "Episode: 57 Total reward: 14.0 Training loss: 5.0733 Explore P: 0.9011\n",
      "Episode: 58 Total reward: 19.0 Training loss: 57.9567 Explore P: 0.8994\n",
      "Episode: 59 Total reward: 10.0 Training loss: 35.5151 Explore P: 0.8985\n",
      "Episode: 60 Total reward: 22.0 Training loss: 29.8473 Explore P: 0.8965\n",
      "Episode: 61 Total reward: 28.0 Training loss: 53.4371 Explore P: 0.8940\n",
      "Episode: 62 Total reward: 10.0 Training loss: 6.7720 Explore P: 0.8932\n",
      "Episode: 63 Total reward: 15.0 Training loss: 136.8991 Explore P: 0.8918\n",
      "Episode: 64 Total reward: 14.0 Training loss: 46.6565 Explore P: 0.8906\n",
      "Episode: 65 Total reward: 22.0 Training loss: 79.3713 Explore P: 0.8887\n",
      "Episode: 66 Total reward: 12.0 Training loss: 35.0083 Explore P: 0.8876\n",
      "Episode: 67 Total reward: 13.0 Training loss: 7.2668 Explore P: 0.8865\n",
      "Episode: 68 Total reward: 58.0 Training loss: 58.2625 Explore P: 0.8814\n",
      "Episode: 69 Total reward: 19.0 Training loss: 11.1489 Explore P: 0.8797\n",
      "Episode: 70 Total reward: 10.0 Training loss: 6.4638 Explore P: 0.8789\n",
      "Episode: 71 Total reward: 9.0 Training loss: 85.1264 Explore P: 0.8781\n",
      "Episode: 72 Total reward: 28.0 Training loss: 34.7985 Explore P: 0.8757\n",
      "Episode: 73 Total reward: 24.0 Training loss: 104.8687 Explore P: 0.8736\n",
      "Episode: 74 Total reward: 12.0 Training loss: 77.0026 Explore P: 0.8726\n",
      "Episode: 75 Total reward: 11.0 Training loss: 11.2387 Explore P: 0.8716\n",
      "Episode: 76 Total reward: 10.0 Training loss: 11.4638 Explore P: 0.8708\n",
      "Episode: 77 Total reward: 11.0 Training loss: 88.9406 Explore P: 0.8698\n",
      "Episode: 78 Total reward: 11.0 Training loss: 82.5079 Explore P: 0.8689\n",
      "Episode: 79 Total reward: 16.0 Training loss: 9.3050 Explore P: 0.8675\n",
      "Episode: 80 Total reward: 20.0 Training loss: 7.9844 Explore P: 0.8658\n",
      "Episode: 81 Total reward: 41.0 Training loss: 244.8609 Explore P: 0.8623\n",
      "Episode: 82 Total reward: 20.0 Training loss: 66.1020 Explore P: 0.8606\n",
      "Episode: 83 Total reward: 9.0 Training loss: 155.0596 Explore P: 0.8598\n",
      "Episode: 84 Total reward: 11.0 Training loss: 56.7468 Explore P: 0.8589\n",
      "Episode: 85 Total reward: 39.0 Training loss: 8.3678 Explore P: 0.8556\n",
      "Episode: 86 Total reward: 9.0 Training loss: 11.6371 Explore P: 0.8548\n",
      "Episode: 87 Total reward: 16.0 Training loss: 8.3508 Explore P: 0.8535\n",
      "Episode: 88 Total reward: 23.0 Training loss: 87.8443 Explore P: 0.8515\n",
      "Episode: 89 Total reward: 20.0 Training loss: 134.1956 Explore P: 0.8498\n",
      "Episode: 90 Total reward: 30.0 Training loss: 58.8765 Explore P: 0.8473\n",
      "Episode: 91 Total reward: 19.0 Training loss: 8.2797 Explore P: 0.8457\n",
      "Episode: 92 Total reward: 44.0 Training loss: 186.5349 Explore P: 0.8421\n",
      "Episode: 93 Total reward: 12.0 Training loss: 249.5852 Explore P: 0.8411\n",
      "Episode: 94 Total reward: 13.0 Training loss: 8.7577 Explore P: 0.8400\n",
      "Episode: 95 Total reward: 11.0 Training loss: 139.6496 Explore P: 0.8391\n",
      "Episode: 96 Total reward: 17.0 Training loss: 8.4133 Explore P: 0.8377\n",
      "Episode: 97 Total reward: 19.0 Training loss: 7.8061 Explore P: 0.8361\n",
      "Episode: 98 Total reward: 9.0 Training loss: 65.1083 Explore P: 0.8353\n",
      "Episode: 99 Total reward: 26.0 Training loss: 187.6543 Explore P: 0.8332\n",
      "Episode: 100 Total reward: 20.0 Training loss: 5.6353 Explore P: 0.8316\n",
      "Episode: 101 Total reward: 30.0 Training loss: 101.9018 Explore P: 0.8291\n",
      "Episode: 102 Total reward: 10.0 Training loss: 173.6712 Explore P: 0.8283\n",
      "Episode: 103 Total reward: 11.0 Training loss: 153.3763 Explore P: 0.8274\n",
      "Episode: 104 Total reward: 25.0 Training loss: 182.9396 Explore P: 0.8253\n",
      "Episode: 105 Total reward: 9.0 Training loss: 70.7137 Explore P: 0.8246\n",
      "Episode: 106 Total reward: 29.0 Training loss: 126.4806 Explore P: 0.8222\n",
      "Episode: 107 Total reward: 21.0 Training loss: 162.5909 Explore P: 0.8205\n",
      "Episode: 108 Total reward: 16.0 Training loss: 230.8139 Explore P: 0.8192\n",
      "Episode: 109 Total reward: 15.0 Training loss: 80.0598 Explore P: 0.8180\n",
      "Episode: 110 Total reward: 22.0 Training loss: 147.6160 Explore P: 0.8163\n",
      "Episode: 111 Total reward: 9.0 Training loss: 70.4135 Explore P: 0.8155\n",
      "Episode: 112 Total reward: 36.0 Training loss: 78.4781 Explore P: 0.8126\n",
      "Episode: 113 Total reward: 16.0 Training loss: 3.6692 Explore P: 0.8114\n",
      "Episode: 114 Total reward: 23.0 Training loss: 80.0994 Explore P: 0.8095\n",
      "Episode: 115 Total reward: 16.0 Training loss: 37.4480 Explore P: 0.8082\n",
      "Episode: 116 Total reward: 9.0 Training loss: 4.3211 Explore P: 0.8075\n",
      "Episode: 117 Total reward: 22.0 Training loss: 182.2126 Explore P: 0.8058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 118 Total reward: 18.0 Training loss: 99.5782 Explore P: 0.8043\n",
      "Episode: 119 Total reward: 20.0 Training loss: 51.8825 Explore P: 0.8027\n",
      "Episode: 120 Total reward: 9.0 Training loss: 2.8001 Explore P: 0.8020\n",
      "Episode: 121 Total reward: 15.0 Training loss: 57.9544 Explore P: 0.8008\n",
      "Episode: 122 Total reward: 12.0 Training loss: 47.3743 Explore P: 0.7999\n",
      "Episode: 123 Total reward: 26.0 Training loss: 42.5793 Explore P: 0.7978\n",
      "Episode: 124 Total reward: 24.0 Training loss: 133.2636 Explore P: 0.7960\n",
      "Episode: 125 Total reward: 21.0 Training loss: 47.8806 Explore P: 0.7943\n",
      "Episode: 126 Total reward: 19.0 Training loss: 4.8982 Explore P: 0.7928\n",
      "Episode: 127 Total reward: 13.0 Training loss: 79.4500 Explore P: 0.7918\n",
      "Episode: 128 Total reward: 27.0 Training loss: 3.1570 Explore P: 0.7897\n",
      "Episode: 129 Total reward: 10.0 Training loss: 3.0235 Explore P: 0.7889\n",
      "Episode: 130 Total reward: 26.0 Training loss: 2.5566 Explore P: 0.7869\n",
      "Episode: 131 Total reward: 14.0 Training loss: 64.2665 Explore P: 0.7858\n",
      "Episode: 132 Total reward: 21.0 Training loss: 1.9459 Explore P: 0.7842\n",
      "Episode: 133 Total reward: 13.0 Training loss: 32.0449 Explore P: 0.7832\n",
      "Episode: 134 Total reward: 13.0 Training loss: 2.9993 Explore P: 0.7822\n",
      "Episode: 135 Total reward: 11.0 Training loss: 2.0728 Explore P: 0.7813\n",
      "Episode: 136 Total reward: 10.0 Training loss: 3.9147 Explore P: 0.7806\n",
      "Episode: 137 Total reward: 21.0 Training loss: 3.4352 Explore P: 0.7789\n",
      "Episode: 138 Total reward: 11.0 Training loss: 33.3648 Explore P: 0.7781\n",
      "Episode: 139 Total reward: 10.0 Training loss: 16.2810 Explore P: 0.7773\n",
      "Episode: 140 Total reward: 10.0 Training loss: 15.8600 Explore P: 0.7766\n",
      "Episode: 141 Total reward: 20.0 Training loss: 4.1962 Explore P: 0.7750\n",
      "Episode: 142 Total reward: 33.0 Training loss: 4.1674 Explore P: 0.7725\n",
      "Episode: 143 Total reward: 11.0 Training loss: 46.3184 Explore P: 0.7717\n",
      "Episode: 144 Total reward: 37.0 Training loss: 31.2699 Explore P: 0.7689\n",
      "Episode: 145 Total reward: 24.0 Training loss: 15.0080 Explore P: 0.7670\n",
      "Episode: 146 Total reward: 17.0 Training loss: 23.2483 Explore P: 0.7657\n",
      "Episode: 147 Total reward: 12.0 Training loss: 13.7009 Explore P: 0.7648\n",
      "Episode: 148 Total reward: 15.0 Training loss: 2.7657 Explore P: 0.7637\n",
      "Episode: 149 Total reward: 12.0 Training loss: 16.7857 Explore P: 0.7628\n",
      "Episode: 150 Total reward: 34.0 Training loss: 14.0678 Explore P: 0.7602\n",
      "Episode: 151 Total reward: 32.0 Training loss: 8.3870 Explore P: 0.7579\n",
      "Episode: 152 Total reward: 14.0 Training loss: 2.6686 Explore P: 0.7568\n",
      "Episode: 153 Total reward: 11.0 Training loss: 15.8789 Explore P: 0.7560\n",
      "Episode: 154 Total reward: 26.0 Training loss: 7.0677 Explore P: 0.7540\n",
      "Episode: 155 Total reward: 11.0 Training loss: 11.1005 Explore P: 0.7532\n",
      "Episode: 156 Total reward: 16.0 Training loss: 2.7297 Explore P: 0.7520\n",
      "Episode: 157 Total reward: 13.0 Training loss: 2.4058 Explore P: 0.7511\n",
      "Episode: 158 Total reward: 18.0 Training loss: 2.2223 Explore P: 0.7497\n",
      "Episode: 159 Total reward: 35.0 Training loss: 11.9182 Explore P: 0.7472\n",
      "Episode: 160 Total reward: 51.0 Training loss: 11.8792 Explore P: 0.7434\n",
      "Episode: 161 Total reward: 43.0 Training loss: 3.8420 Explore P: 0.7403\n",
      "Episode: 162 Total reward: 20.0 Training loss: 30.0362 Explore P: 0.7388\n",
      "Episode: 163 Total reward: 35.0 Training loss: 2.0680 Explore P: 0.7363\n",
      "Episode: 164 Total reward: 16.0 Training loss: 17.8198 Explore P: 0.7351\n",
      "Episode: 165 Total reward: 47.0 Training loss: 28.7343 Explore P: 0.7317\n",
      "Episode: 166 Total reward: 14.0 Training loss: 13.9143 Explore P: 0.7307\n",
      "Episode: 167 Total reward: 44.0 Training loss: 2.7026 Explore P: 0.7275\n",
      "Episode: 168 Total reward: 18.0 Training loss: 17.2439 Explore P: 0.7262\n",
      "Episode: 169 Total reward: 35.0 Training loss: 2.4387 Explore P: 0.7237\n",
      "Episode: 170 Total reward: 27.0 Training loss: 13.2532 Explore P: 0.7218\n",
      "Episode: 171 Total reward: 58.0 Training loss: 55.8100 Explore P: 0.7177\n",
      "Episode: 172 Total reward: 11.0 Training loss: 1.9210 Explore P: 0.7169\n",
      "Episode: 173 Total reward: 38.0 Training loss: 2.7349 Explore P: 0.7142\n",
      "Episode: 174 Total reward: 73.0 Training loss: 25.3100 Explore P: 0.7091\n",
      "Episode: 175 Total reward: 41.0 Training loss: 5.9190 Explore P: 0.7062\n",
      "Episode: 176 Total reward: 66.0 Training loss: 40.1083 Explore P: 0.7017\n",
      "Episode: 177 Total reward: 59.0 Training loss: 3.2331 Explore P: 0.6976\n",
      "Episode: 178 Total reward: 21.0 Training loss: 2.8475 Explore P: 0.6962\n",
      "Episode: 179 Total reward: 20.0 Training loss: 20.0153 Explore P: 0.6948\n",
      "Episode: 180 Total reward: 24.0 Training loss: 9.5072 Explore P: 0.6931\n",
      "Episode: 181 Total reward: 21.0 Training loss: 10.2654 Explore P: 0.6917\n",
      "Episode: 182 Total reward: 22.0 Training loss: 2.8355 Explore P: 0.6902\n",
      "Episode: 183 Total reward: 12.0 Training loss: 12.1894 Explore P: 0.6894\n",
      "Episode: 184 Total reward: 47.0 Training loss: 103.5765 Explore P: 0.6862\n",
      "Episode: 185 Total reward: 42.0 Training loss: 5.1058 Explore P: 0.6834\n",
      "Episode: 186 Total reward: 40.0 Training loss: 32.3611 Explore P: 0.6807\n",
      "Episode: 187 Total reward: 15.0 Training loss: 3.4050 Explore P: 0.6797\n",
      "Episode: 188 Total reward: 28.0 Training loss: 2.9533 Explore P: 0.6778\n",
      "Episode: 189 Total reward: 44.0 Training loss: 47.6049 Explore P: 0.6749\n",
      "Episode: 190 Total reward: 16.0 Training loss: 65.7739 Explore P: 0.6738\n",
      "Episode: 191 Total reward: 79.0 Training loss: 34.7320 Explore P: 0.6686\n",
      "Episode: 192 Total reward: 48.0 Training loss: 20.8978 Explore P: 0.6654\n",
      "Episode: 193 Total reward: 29.0 Training loss: 17.2417 Explore P: 0.6635\n",
      "Episode: 194 Total reward: 22.0 Training loss: 1.0683 Explore P: 0.6621\n",
      "Episode: 195 Total reward: 94.0 Training loss: 26.8977 Explore P: 0.6560\n",
      "Episode: 196 Total reward: 11.0 Training loss: 20.9706 Explore P: 0.6553\n",
      "Episode: 197 Total reward: 59.0 Training loss: 2.5065 Explore P: 0.6515\n",
      "Episode: 198 Total reward: 16.0 Training loss: 5.1904 Explore P: 0.6505\n",
      "Episode: 199 Total reward: 26.0 Training loss: 81.2167 Explore P: 0.6488\n",
      "Episode: 200 Total reward: 43.0 Training loss: 1.7574 Explore P: 0.6461\n",
      "Episode: 201 Total reward: 17.0 Training loss: 2.1524 Explore P: 0.6450\n",
      "Episode: 202 Total reward: 41.0 Training loss: 21.5844 Explore P: 0.6424\n",
      "Episode: 203 Total reward: 33.0 Training loss: 47.3033 Explore P: 0.6403\n",
      "Episode: 204 Total reward: 45.0 Training loss: 16.0320 Explore P: 0.6375\n",
      "Episode: 205 Total reward: 19.0 Training loss: 4.1695 Explore P: 0.6363\n",
      "Episode: 206 Total reward: 12.0 Training loss: 23.4882 Explore P: 0.6355\n",
      "Episode: 207 Total reward: 34.0 Training loss: 2.5222 Explore P: 0.6334\n",
      "Episode: 208 Total reward: 56.0 Training loss: 82.0451 Explore P: 0.6299\n",
      "Episode: 209 Total reward: 59.0 Training loss: 103.0751 Explore P: 0.6263\n",
      "Episode: 210 Total reward: 30.0 Training loss: 4.9091 Explore P: 0.6244\n",
      "Episode: 211 Total reward: 24.0 Training loss: 47.7898 Explore P: 0.6230\n",
      "Episode: 212 Total reward: 25.0 Training loss: 49.3633 Explore P: 0.6214\n",
      "Episode: 213 Total reward: 75.0 Training loss: 42.7742 Explore P: 0.6169\n",
      "Episode: 214 Total reward: 49.0 Training loss: 58.2640 Explore P: 0.6139\n",
      "Episode: 215 Total reward: 39.0 Training loss: 4.4875 Explore P: 0.6115\n",
      "Episode: 216 Total reward: 38.0 Training loss: 40.2256 Explore P: 0.6093\n",
      "Episode: 217 Total reward: 12.0 Training loss: 109.2173 Explore P: 0.6085\n",
      "Episode: 218 Total reward: 72.0 Training loss: 3.9325 Explore P: 0.6043\n",
      "Episode: 219 Total reward: 45.0 Training loss: 4.0202 Explore P: 0.6016\n",
      "Episode: 220 Total reward: 26.0 Training loss: 6.7986 Explore P: 0.6000\n",
      "Episode: 221 Total reward: 47.0 Training loss: 7.1463 Explore P: 0.5973\n",
      "Episode: 222 Total reward: 21.0 Training loss: 69.8002 Explore P: 0.5960\n",
      "Episode: 223 Total reward: 15.0 Training loss: 17.5075 Explore P: 0.5952\n",
      "Episode: 224 Total reward: 21.0 Training loss: 8.6084 Explore P: 0.5939\n",
      "Episode: 225 Total reward: 56.0 Training loss: 110.5806 Explore P: 0.5907\n",
      "Episode: 226 Total reward: 32.0 Training loss: 39.5131 Explore P: 0.5888\n",
      "Episode: 227 Total reward: 46.0 Training loss: 108.9952 Explore P: 0.5862\n",
      "Episode: 228 Total reward: 53.0 Training loss: 85.4352 Explore P: 0.5831\n",
      "Episode: 229 Total reward: 135.0 Training loss: 30.4446 Explore P: 0.5754\n",
      "Episode: 230 Total reward: 53.0 Training loss: 10.9517 Explore P: 0.5725\n",
      "Episode: 231 Total reward: 16.0 Training loss: 6.0296 Explore P: 0.5716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 232 Total reward: 44.0 Training loss: 6.3172 Explore P: 0.5691\n",
      "Episode: 233 Total reward: 62.0 Training loss: 10.2099 Explore P: 0.5656\n",
      "Episode: 234 Total reward: 43.0 Training loss: 230.6787 Explore P: 0.5632\n",
      "Episode: 235 Total reward: 22.0 Training loss: 6.0033 Explore P: 0.5620\n",
      "Episode: 236 Total reward: 93.0 Training loss: 13.9603 Explore P: 0.5569\n",
      "Episode: 237 Total reward: 89.0 Training loss: 7.6613 Explore P: 0.5521\n",
      "Episode: 238 Total reward: 31.0 Training loss: 107.6587 Explore P: 0.5504\n",
      "Episode: 239 Total reward: 35.0 Training loss: 115.5463 Explore P: 0.5485\n",
      "Episode: 240 Total reward: 11.0 Training loss: 4.7962 Explore P: 0.5479\n",
      "Episode: 241 Total reward: 44.0 Training loss: 83.1858 Explore P: 0.5456\n",
      "Episode: 242 Total reward: 61.0 Training loss: 6.5043 Explore P: 0.5423\n",
      "Episode: 243 Total reward: 26.0 Training loss: 4.6053 Explore P: 0.5409\n",
      "Episode: 244 Total reward: 48.0 Training loss: 5.9649 Explore P: 0.5384\n",
      "Episode: 245 Total reward: 74.0 Training loss: 1.7463 Explore P: 0.5345\n",
      "Episode: 246 Total reward: 18.0 Training loss: 4.7643 Explore P: 0.5335\n",
      "Episode: 247 Total reward: 73.0 Training loss: 209.2920 Explore P: 0.5297\n",
      "Episode: 248 Total reward: 25.0 Training loss: 7.1064 Explore P: 0.5284\n",
      "Episode: 249 Total reward: 16.0 Training loss: 113.3582 Explore P: 0.5276\n",
      "Episode: 250 Total reward: 101.0 Training loss: 7.0731 Explore P: 0.5224\n",
      "Episode: 251 Total reward: 113.0 Training loss: 6.6473 Explore P: 0.5166\n",
      "Episode: 252 Total reward: 52.0 Training loss: 352.4955 Explore P: 0.5140\n",
      "Episode: 253 Total reward: 149.0 Training loss: 4.6982 Explore P: 0.5066\n",
      "Episode: 254 Total reward: 107.0 Training loss: 6.4248 Explore P: 0.5013\n",
      "Episode: 255 Total reward: 40.0 Training loss: 281.2180 Explore P: 0.4993\n",
      "Episode: 256 Total reward: 20.0 Training loss: 87.6250 Explore P: 0.4983\n",
      "Episode: 257 Total reward: 37.0 Training loss: 105.1820 Explore P: 0.4965\n",
      "Episode: 258 Total reward: 17.0 Training loss: 5.8887 Explore P: 0.4957\n",
      "Episode: 259 Total reward: 42.0 Training loss: 409.0949 Explore P: 0.4937\n",
      "Episode: 260 Total reward: 94.0 Training loss: 5.7662 Explore P: 0.4891\n",
      "Episode: 261 Total reward: 149.0 Training loss: 12.1646 Explore P: 0.4821\n",
      "Episode: 262 Total reward: 177.0 Training loss: 12.0801 Explore P: 0.4738\n",
      "Episode: 263 Total reward: 75.0 Training loss: 11.1261 Explore P: 0.4703\n",
      "Episode: 264 Total reward: 134.0 Training loss: 12.2531 Explore P: 0.4642\n",
      "Episode: 265 Total reward: 33.0 Training loss: 11.8770 Explore P: 0.4627\n",
      "Episode: 266 Total reward: 119.0 Training loss: 152.3788 Explore P: 0.4573\n",
      "Episode: 267 Total reward: 77.0 Training loss: 383.9610 Explore P: 0.4539\n",
      "Episode: 268 Total reward: 94.0 Training loss: 154.8494 Explore P: 0.4497\n",
      "Episode: 269 Total reward: 124.0 Training loss: 4.6162 Explore P: 0.4443\n",
      "Episode: 270 Total reward: 71.0 Training loss: 496.2096 Explore P: 0.4413\n",
      "Episode: 271 Total reward: 27.0 Training loss: 422.6065 Explore P: 0.4401\n",
      "Episode: 272 Total reward: 129.0 Training loss: 7.9420 Explore P: 0.4346\n",
      "Episode: 273 Total reward: 37.0 Training loss: 15.8294 Explore P: 0.4330\n",
      "Episode: 274 Total reward: 125.0 Training loss: 93.8143 Explore P: 0.4278\n",
      "Episode: 275 Total reward: 118.0 Training loss: 227.4657 Explore P: 0.4229\n",
      "Episode: 276 Total reward: 155.0 Training loss: 8.4890 Explore P: 0.4165\n",
      "Episode: 277 Total reward: 71.0 Training loss: 8.1403 Explore P: 0.4136\n",
      "Episode: 278 Total reward: 131.0 Training loss: 15.9542 Explore P: 0.4084\n",
      "Episode: 279 Total reward: 51.0 Training loss: 163.9563 Explore P: 0.4064\n",
      "Episode: 280 Total reward: 24.0 Training loss: 6.0108 Explore P: 0.4054\n",
      "Episode: 281 Total reward: 52.0 Training loss: 4.4267 Explore P: 0.4034\n",
      "Episode: 282 Total reward: 76.0 Training loss: 3.6795 Explore P: 0.4004\n",
      "Episode: 283 Total reward: 92.0 Training loss: 671.9247 Explore P: 0.3968\n",
      "Episode: 284 Total reward: 112.0 Training loss: 4.7530 Explore P: 0.3925\n",
      "Episode: 285 Total reward: 145.0 Training loss: 12.0629 Explore P: 0.3870\n",
      "Episode: 286 Total reward: 57.0 Training loss: 23.0058 Explore P: 0.3848\n",
      "Episode: 287 Total reward: 112.0 Training loss: 65.6684 Explore P: 0.3807\n",
      "Episode: 288 Total reward: 30.0 Training loss: 39.2412 Explore P: 0.3796\n",
      "Episode: 289 Total reward: 61.0 Training loss: 27.2471 Explore P: 0.3773\n",
      "Episode: 290 Total reward: 75.0 Training loss: 4.2393 Explore P: 0.3746\n",
      "Episode: 291 Total reward: 126.0 Training loss: 16.9763 Explore P: 0.3700\n",
      "Episode: 292 Total reward: 158.0 Training loss: 113.7097 Explore P: 0.3644\n",
      "Episode: 293 Total reward: 130.0 Training loss: 564.8456 Explore P: 0.3598\n",
      "Episode: 294 Total reward: 120.0 Training loss: 3.8202 Explore P: 0.3556\n",
      "Episode: 295 Total reward: 135.0 Training loss: 212.2057 Explore P: 0.3510\n",
      "Episode: 296 Total reward: 83.0 Training loss: 563.2208 Explore P: 0.3482\n",
      "Episode: 297 Total reward: 142.0 Training loss: 11.8782 Explore P: 0.3434\n",
      "Episode: 298 Total reward: 173.0 Training loss: 12.1911 Explore P: 0.3377\n",
      "Episode: 299 Total reward: 128.0 Training loss: 16.0999 Explore P: 0.3335\n",
      "Episode: 300 Total reward: 141.0 Training loss: 395.3225 Explore P: 0.3290\n",
      "Episode: 301 Total reward: 44.0 Training loss: 12.5077 Explore P: 0.3276\n",
      "Episode: 302 Total reward: 118.0 Training loss: 124.2440 Explore P: 0.3238\n",
      "Episode: 304 Total reward: 24.0 Training loss: 5.1105 Explore P: 0.3169\n",
      "Episode: 305 Total reward: 22.0 Training loss: 1.5009 Explore P: 0.3162\n",
      "Episode: 306 Total reward: 111.0 Training loss: 6.7554 Explore P: 0.3128\n",
      "Episode: 308 Total reward: 83.0 Training loss: 10.1030 Explore P: 0.3044\n",
      "Episode: 309 Total reward: 135.0 Training loss: 394.0038 Explore P: 0.3004\n",
      "Episode: 310 Total reward: 111.0 Training loss: 493.9376 Explore P: 0.2972\n",
      "Episode: 311 Total reward: 134.0 Training loss: 8.3816 Explore P: 0.2934\n",
      "Episode: 313 Total reward: 77.0 Training loss: 145.7661 Explore P: 0.2857\n",
      "Episode: 315 Total reward: 23.0 Training loss: 15.4140 Explore P: 0.2796\n",
      "Episode: 316 Total reward: 187.0 Training loss: 3.0442 Explore P: 0.2746\n",
      "Episode: 318 Total reward: 5.0 Training loss: 8.4607 Explore P: 0.2692\n",
      "Episode: 319 Total reward: 114.0 Training loss: 9.6409 Explore P: 0.2663\n",
      "Episode: 320 Total reward: 189.0 Training loss: 58.9019 Explore P: 0.2615\n",
      "Episode: 321 Total reward: 133.0 Training loss: 5.5557 Explore P: 0.2582\n",
      "Episode: 322 Total reward: 99.0 Training loss: 12.4311 Explore P: 0.2557\n",
      "Episode: 324 Total reward: 10.0 Training loss: 3.0438 Explore P: 0.2506\n",
      "Episode: 325 Total reward: 196.0 Training loss: 3.0236 Explore P: 0.2459\n",
      "Episode: 327 Total reward: 183.0 Training loss: 2.0242 Explore P: 0.2371\n",
      "Episode: 329 Total reward: 43.0 Training loss: 2.7680 Explore P: 0.2316\n",
      "Episode: 330 Total reward: 115.0 Training loss: 3.7536 Explore P: 0.2291\n",
      "Episode: 331 Total reward: 181.0 Training loss: 2.0482 Explore P: 0.2252\n",
      "Episode: 333 Total reward: 47.0 Training loss: 2.6194 Explore P: 0.2199\n",
      "Episode: 335 Total reward: 89.0 Training loss: 3.0020 Explore P: 0.2139\n",
      "Episode: 336 Total reward: 172.0 Training loss: 5.5402 Explore P: 0.2105\n",
      "Episode: 338 Total reward: 41.0 Training loss: 3.2262 Explore P: 0.2057\n",
      "Episode: 340 Total reward: 27.0 Training loss: 3.8485 Explore P: 0.2013\n",
      "Episode: 342 Total reward: 25.0 Training loss: 4.4657 Explore P: 0.1970\n",
      "Episode: 343 Total reward: 189.0 Training loss: 0.8392 Explore P: 0.1935\n",
      "Episode: 344 Total reward: 167.0 Training loss: 1.1583 Explore P: 0.1905\n",
      "Episode: 346 Total reward: 169.0 Training loss: 1.6194 Explore P: 0.1840\n",
      "Episode: 348 Total reward: 26.0 Training loss: 1.4590 Explore P: 0.1801\n",
      "Episode: 350 Total reward: 119.0 Training loss: 3.4626 Explore P: 0.1747\n",
      "Episode: 352 Total reward: 45.0 Training loss: 4.8541 Explore P: 0.1707\n",
      "Episode: 353 Total reward: 162.0 Training loss: 0.7716 Explore P: 0.1682\n",
      "Episode: 355 Total reward: 25.0 Training loss: 0.9298 Explore P: 0.1646\n",
      "Episode: 357 Total reward: 2.0 Training loss: 1.2996 Explore P: 0.1615\n",
      "Episode: 358 Total reward: 187.0 Training loss: 1.3411 Explore P: 0.1587\n",
      "Episode: 359 Total reward: 199.0 Training loss: 0.5827 Explore P: 0.1558\n",
      "Episode: 360 Total reward: 188.0 Training loss: 1.5828 Explore P: 0.1531\n",
      "Episode: 361 Total reward: 198.0 Training loss: 1.2338 Explore P: 0.1503\n",
      "Episode: 363 Total reward: 9.0 Training loss: 1.4026 Explore P: 0.1474\n",
      "Episode: 365 Total reward: 20.0 Training loss: 1.5594 Explore P: 0.1444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 367 Total reward: 1.0 Training loss: 0.8951 Explore P: 0.1417\n",
      "Episode: 368 Total reward: 193.0 Training loss: 0.6430 Explore P: 0.1392\n",
      "Episode: 370 Total reward: 64.0 Training loss: 1.5737 Explore P: 0.1358\n",
      "Episode: 372 Total reward: 92.0 Training loss: 1.0005 Explore P: 0.1322\n",
      "Episode: 374 Total reward: 52.0 Training loss: 0.7822 Explore P: 0.1292\n",
      "Episode: 377 Total reward: 4.0 Training loss: 1.5270 Explore P: 0.1245\n",
      "Episode: 379 Total reward: 14.0 Training loss: 0.9137 Explore P: 0.1220\n",
      "Episode: 381 Total reward: 32.0 Training loss: 0.5850 Explore P: 0.1195\n",
      "Episode: 383 Total reward: 108.0 Training loss: 0.9242 Explore P: 0.1161\n",
      "Episode: 385 Total reward: 144.0 Training loss: 0.4437 Explore P: 0.1126\n",
      "Episode: 387 Total reward: 93.0 Training loss: 0.4372 Explore P: 0.1096\n",
      "Episode: 389 Total reward: 183.0 Training loss: 0.2825 Explore P: 0.1059\n",
      "Episode: 391 Total reward: 34.0 Training loss: 0.3983 Explore P: 0.1036\n",
      "Episode: 393 Total reward: 87.0 Training loss: 0.5882 Explore P: 0.1010\n",
      "Episode: 395 Total reward: 86.0 Training loss: 0.3918 Explore P: 0.0984\n",
      "Episode: 397 Total reward: 53.0 Training loss: 0.5315 Explore P: 0.0962\n",
      "Episode: 400 Total reward: 99.0 Training loss: 0.1844 Explore P: 0.0920\n",
      "Episode: 402 Total reward: 134.0 Training loss: 1.7531 Explore P: 0.0893\n",
      "Episode: 404 Total reward: 32.0 Training loss: 0.4380 Explore P: 0.0875\n",
      "Episode: 407 Total reward: 99.0 Training loss: 0.4307 Explore P: 0.0837\n",
      "Episode: 409 Total reward: 196.0 Training loss: 0.5115 Explore P: 0.0809\n",
      "Episode: 411 Total reward: 62.0 Training loss: 0.1809 Explore P: 0.0790\n",
      "Episode: 414 Total reward: 99.0 Training loss: 0.5540 Explore P: 0.0757\n",
      "Episode: 417 Total reward: 99.0 Training loss: 15.4215 Explore P: 0.0725\n",
      "Episode: 420 Total reward: 69.0 Training loss: 0.5951 Explore P: 0.0696\n",
      "Episode: 423 Total reward: 99.0 Training loss: 0.1438 Explore P: 0.0667\n",
      "Episode: 426 Total reward: 64.0 Training loss: 0.0982 Explore P: 0.0641\n",
      "Episode: 429 Total reward: 99.0 Training loss: 0.3035 Explore P: 0.0615\n",
      "Episode: 432 Total reward: 99.0 Training loss: 0.1339 Explore P: 0.0590\n",
      "Episode: 434 Total reward: 172.0 Training loss: 0.1250 Explore P: 0.0572\n",
      "Episode: 437 Total reward: 99.0 Training loss: 0.2205 Explore P: 0.0549\n",
      "Episode: 439 Total reward: 106.0 Training loss: 0.1040 Explore P: 0.0536\n",
      "Episode: 442 Total reward: 67.0 Training loss: 2.0809 Explore P: 0.0516\n",
      "Episode: 445 Total reward: 99.0 Training loss: 0.0967 Explore P: 0.0495\n",
      "Episode: 448 Total reward: 99.0 Training loss: 0.1009 Explore P: 0.0476\n",
      "Episode: 451 Total reward: 99.0 Training loss: 0.1974 Explore P: 0.0458\n",
      "Episode: 453 Total reward: 189.0 Training loss: 0.0687 Explore P: 0.0444\n",
      "Episode: 456 Total reward: 99.0 Training loss: 0.1110 Explore P: 0.0428\n",
      "Episode: 459 Total reward: 37.0 Training loss: 0.0648 Explore P: 0.0413\n",
      "Episode: 462 Total reward: 99.0 Training loss: 0.0882 Explore P: 0.0398\n",
      "Episode: 465 Total reward: 99.0 Training loss: 0.0919 Explore P: 0.0384\n",
      "Episode: 468 Total reward: 99.0 Training loss: 0.0732 Explore P: 0.0370\n",
      "Episode: 471 Total reward: 99.0 Training loss: 0.0842 Explore P: 0.0357\n",
      "Episode: 474 Total reward: 98.0 Training loss: 0.1260 Explore P: 0.0344\n",
      "Episode: 477 Total reward: 99.0 Training loss: 0.4020 Explore P: 0.0332\n",
      "Episode: 480 Total reward: 99.0 Training loss: 0.0796 Explore P: 0.0321\n",
      "Episode: 483 Total reward: 31.0 Training loss: 0.0686 Explore P: 0.0312\n",
      "Episode: 486 Total reward: 99.0 Training loss: 0.1019 Explore P: 0.0301\n",
      "Episode: 489 Total reward: 99.0 Training loss: 0.1022 Explore P: 0.0292\n",
      "Episode: 492 Total reward: 99.0 Training loss: 0.1229 Explore P: 0.0282\n",
      "Episode: 495 Total reward: 99.0 Training loss: 327.1610 Explore P: 0.0273\n",
      "Episode: 498 Total reward: 99.0 Training loss: 0.1017 Explore P: 0.0265\n",
      "Episode: 501 Total reward: 99.0 Training loss: 0.1280 Explore P: 0.0257\n",
      "Episode: 504 Total reward: 14.0 Training loss: 0.0753 Explore P: 0.0251\n",
      "Episode: 507 Total reward: 99.0 Training loss: 0.1488 Explore P: 0.0243\n",
      "Episode: 510 Total reward: 67.0 Training loss: 0.0456 Explore P: 0.0237\n",
      "Episode: 513 Total reward: 99.0 Training loss: 0.0782 Explore P: 0.0230\n",
      "Episode: 516 Total reward: 99.0 Training loss: 0.0791 Explore P: 0.0224\n",
      "Episode: 519 Total reward: 99.0 Training loss: 0.1196 Explore P: 0.0218\n",
      "Episode: 522 Total reward: 99.0 Training loss: 0.0968 Explore P: 0.0212\n",
      "Episode: 525 Total reward: 99.0 Training loss: 0.0607 Explore P: 0.0207\n",
      "Episode: 528 Total reward: 99.0 Training loss: 0.0669 Explore P: 0.0201\n",
      "Episode: 531 Total reward: 99.0 Training loss: 0.1168 Explore P: 0.0196\n",
      "Episode: 534 Total reward: 99.0 Training loss: 0.1661 Explore P: 0.0192\n",
      "Episode: 537 Total reward: 99.0 Training loss: 0.0657 Explore P: 0.0187\n",
      "Episode: 540 Total reward: 42.0 Training loss: 0.0748 Explore P: 0.0183\n",
      "Episode: 543 Total reward: 99.0 Training loss: 0.0767 Explore P: 0.0179\n",
      "Episode: 546 Total reward: 99.0 Training loss: 0.0982 Explore P: 0.0176\n",
      "Episode: 549 Total reward: 99.0 Training loss: 0.1701 Explore P: 0.0172\n",
      "Episode: 552 Total reward: 99.0 Training loss: 0.0931 Explore P: 0.0168\n",
      "Episode: 555 Total reward: 99.0 Training loss: 0.1276 Explore P: 0.0165\n",
      "Episode: 558 Total reward: 99.0 Training loss: 0.0721 Explore P: 0.0162\n",
      "Episode: 561 Total reward: 99.0 Training loss: 0.1042 Explore P: 0.0159\n",
      "Episode: 564 Total reward: 99.0 Training loss: 0.1039 Explore P: 0.0156\n",
      "Episode: 567 Total reward: 99.0 Training loss: 0.1398 Explore P: 0.0153\n",
      "Episode: 570 Total reward: 99.0 Training loss: 0.0893 Explore P: 0.0151\n",
      "Episode: 573 Total reward: 99.0 Training loss: 0.3126 Explore P: 0.0148\n",
      "Episode: 576 Total reward: 99.0 Training loss: 149.9385 Explore P: 0.0146\n",
      "Episode: 579 Total reward: 99.0 Training loss: 0.3204 Explore P: 0.0144\n",
      "Episode: 582 Total reward: 99.0 Training loss: 0.1965 Explore P: 0.0142\n",
      "Episode: 585 Total reward: 99.0 Training loss: 0.1020 Explore P: 0.0139\n",
      "Episode: 588 Total reward: 99.0 Training loss: 0.0454 Explore P: 0.0138\n",
      "Episode: 591 Total reward: 99.0 Training loss: 0.0812 Explore P: 0.0136\n",
      "Episode: 594 Total reward: 99.0 Training loss: 0.1325 Explore P: 0.0134\n",
      "Episode: 597 Total reward: 99.0 Training loss: 427.8971 Explore P: 0.0132\n",
      "Episode: 600 Total reward: 99.0 Training loss: 0.1842 Explore P: 0.0131\n",
      "Episode: 603 Total reward: 99.0 Training loss: 0.0759 Explore P: 0.0129\n",
      "Episode: 606 Total reward: 99.0 Training loss: 0.1349 Explore P: 0.0128\n",
      "Episode: 609 Total reward: 99.0 Training loss: 0.1995 Explore P: 0.0126\n",
      "Episode: 612 Total reward: 99.0 Training loss: 0.0820 Explore P: 0.0125\n",
      "Episode: 615 Total reward: 99.0 Training loss: 0.0943 Explore P: 0.0124\n",
      "Episode: 618 Total reward: 99.0 Training loss: 0.1063 Explore P: 0.0123\n",
      "Episode: 621 Total reward: 99.0 Training loss: 0.1269 Explore P: 0.0122\n",
      "Episode: 624 Total reward: 35.0 Training loss: 0.1179 Explore P: 0.0121\n",
      "Episode: 627 Total reward: 99.0 Training loss: 0.0979 Explore P: 0.0120\n",
      "Episode: 630 Total reward: 99.0 Training loss: 0.1024 Explore P: 0.0119\n",
      "Episode: 633 Total reward: 99.0 Training loss: 0.0940 Explore P: 0.0118\n",
      "Episode: 636 Total reward: 99.0 Training loss: 0.1297 Explore P: 0.0117\n",
      "Episode: 639 Total reward: 99.0 Training loss: 0.0935 Explore P: 0.0116\n",
      "Episode: 642 Total reward: 99.0 Training loss: 0.2696 Explore P: 0.0115\n",
      "Episode: 645 Total reward: 99.0 Training loss: 0.1328 Explore P: 0.0115\n",
      "Episode: 648 Total reward: 99.0 Training loss: 0.0529 Explore P: 0.0114\n",
      "Episode: 651 Total reward: 99.0 Training loss: 0.1291 Explore P: 0.0113\n",
      "Episode: 654 Total reward: 99.0 Training loss: 0.2366 Explore P: 0.0113\n",
      "Episode: 657 Total reward: 99.0 Training loss: 0.1191 Explore P: 0.0112\n",
      "Episode: 660 Total reward: 99.0 Training loss: 0.0996 Explore P: 0.0111\n",
      "Episode: 663 Total reward: 99.0 Training loss: 0.0963 Explore P: 0.0111\n",
      "Episode: 666 Total reward: 99.0 Training loss: 0.1334 Explore P: 0.0110\n",
      "Episode: 669 Total reward: 99.0 Training loss: 0.2196 Explore P: 0.0110\n",
      "Episode: 672 Total reward: 99.0 Training loss: 0.0671 Explore P: 0.0109\n",
      "Episode: 675 Total reward: 99.0 Training loss: 0.1231 Explore P: 0.0109\n",
      "Episode: 678 Total reward: 99.0 Training loss: 0.0895 Explore P: 0.0108\n",
      "Episode: 681 Total reward: 99.0 Training loss: 0.0721 Explore P: 0.0108\n",
      "Episode: 684 Total reward: 99.0 Training loss: 0.2434 Explore P: 0.0108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 687 Total reward: 99.0 Training loss: 446.3375 Explore P: 0.0107\n",
      "Episode: 690 Total reward: 99.0 Training loss: 0.1281 Explore P: 0.0107\n",
      "Episode: 693 Total reward: 99.0 Training loss: 0.1117 Explore P: 0.0107\n",
      "Episode: 696 Total reward: 99.0 Training loss: 0.1171 Explore P: 0.0106\n",
      "Episode: 699 Total reward: 99.0 Training loss: 0.1853 Explore P: 0.0106\n",
      "Episode: 702 Total reward: 99.0 Training loss: 0.1859 Explore P: 0.0106\n",
      "Episode: 705 Total reward: 99.0 Training loss: 0.1506 Explore P: 0.0105\n",
      "Episode: 708 Total reward: 99.0 Training loss: 0.1329 Explore P: 0.0105\n",
      "Episode: 711 Total reward: 99.0 Training loss: 0.2049 Explore P: 0.0105\n",
      "Episode: 714 Total reward: 99.0 Training loss: 346.6513 Explore P: 0.0105\n",
      "Episode: 717 Total reward: 99.0 Training loss: 0.0832 Explore P: 0.0104\n",
      "Episode: 720 Total reward: 99.0 Training loss: 0.1362 Explore P: 0.0104\n",
      "Episode: 723 Total reward: 99.0 Training loss: 0.0562 Explore P: 0.0104\n",
      "Episode: 726 Total reward: 99.0 Training loss: 0.1340 Explore P: 0.0104\n",
      "Episode: 728 Total reward: 86.0 Training loss: 0.1282 Explore P: 0.0104\n",
      "Episode: 730 Total reward: 105.0 Training loss: 0.2974 Explore P: 0.0104\n",
      "Episode: 733 Total reward: 99.0 Training loss: 0.1583 Explore P: 0.0103\n",
      "Episode: 735 Total reward: 160.0 Training loss: 0.0454 Explore P: 0.0103\n",
      "Episode: 738 Total reward: 99.0 Training loss: 0.2669 Explore P: 0.0103\n",
      "Episode: 741 Total reward: 99.0 Training loss: 0.1051 Explore P: 0.0103\n",
      "Episode: 744 Total reward: 99.0 Training loss: 0.2562 Explore P: 0.0103\n",
      "Episode: 747 Total reward: 99.0 Training loss: 0.0681 Explore P: 0.0103\n",
      "Episode: 750 Total reward: 99.0 Training loss: 0.1605 Explore P: 0.0103\n",
      "Episode: 753 Total reward: 99.0 Training loss: 0.2197 Explore P: 0.0102\n",
      "Episode: 756 Total reward: 99.0 Training loss: 0.0874 Explore P: 0.0102\n",
      "Episode: 759 Total reward: 99.0 Training loss: 0.1157 Explore P: 0.0102\n",
      "Episode: 762 Total reward: 99.0 Training loss: 0.1113 Explore P: 0.0102\n",
      "Episode: 765 Total reward: 99.0 Training loss: 0.4488 Explore P: 0.0102\n",
      "Episode: 768 Total reward: 99.0 Training loss: 0.3378 Explore P: 0.0102\n",
      "Episode: 771 Total reward: 99.0 Training loss: 0.4212 Explore P: 0.0102\n",
      "Episode: 774 Total reward: 99.0 Training loss: 0.2424 Explore P: 0.0102\n",
      "Episode: 777 Total reward: 99.0 Training loss: 1.7931 Explore P: 0.0102\n",
      "Episode: 780 Total reward: 99.0 Training loss: 0.4259 Explore P: 0.0102\n",
      "Episode: 783 Total reward: 99.0 Training loss: 0.9356 Explore P: 0.0101\n",
      "Episode: 785 Total reward: 42.0 Training loss: 1.3922 Explore P: 0.0101\n",
      "Episode: 786 Total reward: 155.0 Training loss: 0.8577 Explore P: 0.0101\n",
      "Episode: 787 Total reward: 130.0 Training loss: 0.8472 Explore P: 0.0101\n",
      "Episode: 788 Total reward: 100.0 Training loss: 1.0605 Explore P: 0.0101\n",
      "Episode: 789 Total reward: 87.0 Training loss: 2.6933 Explore P: 0.0101\n",
      "Episode: 790 Total reward: 51.0 Training loss: 1.4011 Explore P: 0.0101\n",
      "Episode: 791 Total reward: 55.0 Training loss: 1.5708 Explore P: 0.0101\n",
      "Episode: 792 Total reward: 53.0 Training loss: 2.9687 Explore P: 0.0101\n",
      "Episode: 793 Total reward: 48.0 Training loss: 2.4528 Explore P: 0.0101\n",
      "Episode: 794 Total reward: 50.0 Training loss: 1.1898 Explore P: 0.0101\n",
      "Episode: 795 Total reward: 46.0 Training loss: 1.4649 Explore P: 0.0101\n",
      "Episode: 796 Total reward: 46.0 Training loss: 2.2921 Explore P: 0.0101\n",
      "Episode: 797 Total reward: 37.0 Training loss: 3.4553 Explore P: 0.0101\n",
      "Episode: 798 Total reward: 19.0 Training loss: 2.6756 Explore P: 0.0101\n",
      "Episode: 799 Total reward: 12.0 Training loss: 1.6575 Explore P: 0.0101\n",
      "Episode: 800 Total reward: 11.0 Training loss: 19.1417 Explore P: 0.0101\n",
      "Episode: 801 Total reward: 12.0 Training loss: 1.4360 Explore P: 0.0101\n",
      "Episode: 802 Total reward: 15.0 Training loss: 3.1929 Explore P: 0.0101\n",
      "Episode: 803 Total reward: 31.0 Training loss: 7.5489 Explore P: 0.0101\n",
      "Episode: 804 Total reward: 32.0 Training loss: 4.2145 Explore P: 0.0101\n",
      "Episode: 805 Total reward: 23.0 Training loss: 4.2090 Explore P: 0.0101\n",
      "Episode: 806 Total reward: 45.0 Training loss: 1.6142 Explore P: 0.0101\n",
      "Episode: 807 Total reward: 38.0 Training loss: 3.3009 Explore P: 0.0101\n",
      "Episode: 808 Total reward: 56.0 Training loss: 3.4856 Explore P: 0.0101\n",
      "Episode: 811 Total reward: 99.0 Training loss: 0.9512 Explore P: 0.0101\n",
      "Episode: 814 Total reward: 99.0 Training loss: 5.9885 Explore P: 0.0101\n",
      "Episode: 816 Total reward: 152.0 Training loss: 4.5182 Explore P: 0.0101\n",
      "Episode: 817 Total reward: 150.0 Training loss: 7.1150 Explore P: 0.0101\n",
      "Episode: 818 Total reward: 15.0 Training loss: 4.7412 Explore P: 0.0101\n",
      "Episode: 819 Total reward: 12.0 Training loss: 4.3340 Explore P: 0.0101\n",
      "Episode: 820 Total reward: 16.0 Training loss: 7.0594 Explore P: 0.0101\n",
      "Episode: 821 Total reward: 12.0 Training loss: 4.5310 Explore P: 0.0101\n",
      "Episode: 822 Total reward: 13.0 Training loss: 8.0740 Explore P: 0.0101\n",
      "Episode: 823 Total reward: 12.0 Training loss: 7.4907 Explore P: 0.0101\n",
      "Episode: 824 Total reward: 13.0 Training loss: 5.5547 Explore P: 0.0101\n",
      "Episode: 825 Total reward: 10.0 Training loss: 131.5432 Explore P: 0.0101\n",
      "Episode: 826 Total reward: 13.0 Training loss: 11.1664 Explore P: 0.0101\n",
      "Episode: 827 Total reward: 9.0 Training loss: 5.5943 Explore P: 0.0101\n",
      "Episode: 828 Total reward: 10.0 Training loss: 7.6485 Explore P: 0.0101\n",
      "Episode: 829 Total reward: 12.0 Training loss: 5.9441 Explore P: 0.0101\n",
      "Episode: 830 Total reward: 12.0 Training loss: 6.3902 Explore P: 0.0101\n",
      "Episode: 831 Total reward: 13.0 Training loss: 17.4310 Explore P: 0.0101\n",
      "Episode: 832 Total reward: 12.0 Training loss: 6.8424 Explore P: 0.0101\n",
      "Episode: 833 Total reward: 12.0 Training loss: 3.2965 Explore P: 0.0101\n",
      "Episode: 834 Total reward: 12.0 Training loss: 4.0280 Explore P: 0.0101\n",
      "Episode: 835 Total reward: 15.0 Training loss: 6.5461 Explore P: 0.0101\n",
      "Episode: 838 Total reward: 99.0 Training loss: 2.3998 Explore P: 0.0101\n",
      "Episode: 841 Total reward: 99.0 Training loss: 4.3237 Explore P: 0.0101\n",
      "Episode: 844 Total reward: 99.0 Training loss: 7.2840 Explore P: 0.0101\n",
      "Episode: 847 Total reward: 99.0 Training loss: 2.0944 Explore P: 0.0101\n",
      "Episode: 850 Total reward: 99.0 Training loss: 4.0209 Explore P: 0.0101\n",
      "Episode: 853 Total reward: 99.0 Training loss: 2.9160 Explore P: 0.0101\n",
      "Episode: 856 Total reward: 99.0 Training loss: 5.5840 Explore P: 0.0101\n",
      "Episode: 859 Total reward: 92.0 Training loss: 11.7806 Explore P: 0.0101\n",
      "Episode: 862 Total reward: 92.0 Training loss: 1.7122 Explore P: 0.0101\n",
      "Episode: 865 Total reward: 36.0 Training loss: 1.8515 Explore P: 0.0101\n",
      "Episode: 868 Total reward: 99.0 Training loss: 10.3270 Explore P: 0.0101\n",
      "Episode: 871 Total reward: 21.0 Training loss: 670.3339 Explore P: 0.0101\n",
      "Episode: 874 Total reward: 99.0 Training loss: 5.4972 Explore P: 0.0101\n",
      "Episode: 877 Total reward: 12.0 Training loss: 20.2976 Explore P: 0.0101\n",
      "Episode: 880 Total reward: 60.0 Training loss: 2.5975 Explore P: 0.0101\n",
      "Episode: 883 Total reward: 99.0 Training loss: 1.8005 Explore P: 0.0101\n",
      "Episode: 886 Total reward: 99.0 Training loss: 1.5956 Explore P: 0.0100\n",
      "Episode: 889 Total reward: 99.0 Training loss: 0.8984 Explore P: 0.0100\n",
      "Episode: 892 Total reward: 99.0 Training loss: 1.3734 Explore P: 0.0100\n",
      "Episode: 895 Total reward: 99.0 Training loss: 1.5590 Explore P: 0.0100\n",
      "Episode: 898 Total reward: 99.0 Training loss: 84.6337 Explore P: 0.0100\n",
      "Episode: 901 Total reward: 99.0 Training loss: 0.8410 Explore P: 0.0100\n",
      "Episode: 904 Total reward: 99.0 Training loss: 0.6738 Explore P: 0.0100\n",
      "Episode: 907 Total reward: 99.0 Training loss: 0.6894 Explore P: 0.0100\n",
      "Episode: 910 Total reward: 99.0 Training loss: 0.6620 Explore P: 0.0100\n",
      "Episode: 913 Total reward: 99.0 Training loss: 0.3968 Explore P: 0.0100\n",
      "Episode: 916 Total reward: 99.0 Training loss: 0.4447 Explore P: 0.0100\n",
      "Episode: 919 Total reward: 99.0 Training loss: 0.6824 Explore P: 0.0100\n",
      "Episode: 922 Total reward: 99.0 Training loss: 1.8533 Explore P: 0.0100\n",
      "Episode: 924 Total reward: 192.0 Training loss: 0.6576 Explore P: 0.0100\n",
      "Episode: 926 Total reward: 181.0 Training loss: 0.4906 Explore P: 0.0100\n",
      "Episode: 929 Total reward: 87.0 Training loss: 0.3584 Explore P: 0.0100\n",
      "Episode: 932 Total reward: 65.0 Training loss: 0.4540 Explore P: 0.0100\n",
      "Episode: 935 Total reward: 47.0 Training loss: 0.6159 Explore P: 0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 937 Total reward: 112.0 Training loss: 0.8557 Explore P: 0.0100\n",
      "Episode: 939 Total reward: 184.0 Training loss: 0.5009 Explore P: 0.0100\n",
      "Episode: 941 Total reward: 183.0 Training loss: 151.3902 Explore P: 0.0100\n",
      "Episode: 944 Total reward: 23.0 Training loss: 1.5595 Explore P: 0.0100\n",
      "Episode: 946 Total reward: 101.0 Training loss: 1.6290 Explore P: 0.0100\n",
      "Episode: 948 Total reward: 132.0 Training loss: 0.4186 Explore P: 0.0100\n",
      "Episode: 950 Total reward: 169.0 Training loss: 0.9073 Explore P: 0.0100\n",
      "Episode: 952 Total reward: 193.0 Training loss: 1.0044 Explore P: 0.0100\n",
      "Episode: 954 Total reward: 162.0 Training loss: 0.6435 Explore P: 0.0100\n",
      "Episode: 956 Total reward: 99.0 Training loss: 1.1068 Explore P: 0.0100\n",
      "Episode: 958 Total reward: 105.0 Training loss: 1.1698 Explore P: 0.0100\n",
      "Episode: 960 Total reward: 147.0 Training loss: 1.8471 Explore P: 0.0100\n",
      "Episode: 962 Total reward: 138.0 Training loss: 0.2234 Explore P: 0.0100\n",
      "Episode: 964 Total reward: 193.0 Training loss: 0.8223 Explore P: 0.0100\n",
      "Episode: 967 Total reward: 1.0 Training loss: 0.5253 Explore P: 0.0100\n",
      "Episode: 969 Total reward: 143.0 Training loss: 0.9064 Explore P: 0.0100\n",
      "Episode: 971 Total reward: 118.0 Training loss: 0.6171 Explore P: 0.0100\n",
      "Episode: 973 Total reward: 85.0 Training loss: 0.3195 Explore P: 0.0100\n",
      "Episode: 975 Total reward: 88.0 Training loss: 1.1958 Explore P: 0.0100\n",
      "Episode: 977 Total reward: 111.0 Training loss: 0.4242 Explore P: 0.0100\n",
      "Episode: 979 Total reward: 113.0 Training loss: 0.1868 Explore P: 0.0100\n",
      "Episode: 981 Total reward: 164.0 Training loss: 0.3512 Explore P: 0.0100\n",
      "Episode: 983 Total reward: 58.0 Training loss: 0.6666 Explore P: 0.0100\n",
      "Episode: 985 Total reward: 144.0 Training loss: 48.9174 Explore P: 0.0100\n",
      "Episode: 987 Total reward: 112.0 Training loss: 0.2267 Explore P: 0.0100\n",
      "Episode: 989 Total reward: 126.0 Training loss: 0.3029 Explore P: 0.0100\n",
      "Episode: 991 Total reward: 96.0 Training loss: 0.1533 Explore P: 0.0100\n",
      "Episode: 993 Total reward: 149.0 Training loss: 0.1337 Explore P: 0.0100\n",
      "Episode: 995 Total reward: 131.0 Training loss: 6.6655 Explore P: 0.0100\n",
      "Episode: 997 Total reward: 137.0 Training loss: 0.2269 Explore P: 0.0100\n",
      "Episode: 999 Total reward: 114.0 Training loss: 0.0602 Explore P: 0.0100\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()        \n",
    "rewards_list = []\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True,allow_soft_placement=False)) as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    step = 0\n",
    "    for ep in range(1, train_episodes):\n",
    "        total_reward = 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            # Uncomment this next line to watch the training\n",
    "            #env.render() \n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from Q-network\n",
    "                feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "                Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                action = np.argmax(Qs)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training loss: {:.4f}'.format(loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            # Train network\n",
    "            target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n",
    "            \n",
    "            # Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            target_Qs[episode_ends] = (0, 0)\n",
    "            \n",
    "            targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
    "\n",
    "            loss, _ = sess.run([mainQN.loss, mainQN.opt],\n",
    "                                feed_dict={mainQN.inputs_: states,\n",
    "                                           mainQN.targetQs_: targets,\n",
    "                                           mainQN.actions_: actions})\n",
    "        \n",
    "    saver.save(sess, \"checkpoints/cartpole.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Episode: 1 Total reward: 13.0 Training loss: 1.0202 Explore P: 0.9987\n",
    "Episode: 2 Total reward: 13.0 Training loss: 1.0752 Explore P: 0.9974\n",
    "Episode: 500 Total reward: 147.0 Training loss: 0.5124 Explore P: 0.1652\n",
    "Episode: 501 Total reward: 166.0 Training loss: 0.9444 Explore P: 0.1627\n",
    "Episode: 996 Total reward: 99.0 Training loss: 0.1037 Explore P: 0.0101\n",
    "Episode: 997 Total reward: 45.0 Training loss: 0.0619 Explore P: 0.0101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training\n",
    "\n",
    "Below we plot the total rewards for each episode. The rolling average is plotted in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, rews = np.array(rewards_list).T\n",
    "smoothed_rews = running_mean(rews, 10)\n",
    "plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text(0,0.5,'Total Reward')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_21_1.png)\n",
    "\n",
    "\n",
    "## Playing Atari Games\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
